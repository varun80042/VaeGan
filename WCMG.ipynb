{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8052982,"sourceType":"datasetVersion","datasetId":4749266}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install tensorflow keras music21","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-21T12:02:46.069092Z","iopub.execute_input":"2024-04-21T12:02:46.069496Z","iopub.status.idle":"2024-04-21T12:03:10.490153Z","shell.execute_reply.started":"2024-04-21T12:02:46.069465Z","shell.execute_reply":"2024-04-21T12:03:10.489024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Albeniz","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/albeniz\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_albeniz.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"albenizgenerator_model.h5\")\n    gan.discriminator.save(\"albenizdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:04:21.200028Z","iopub.execute_input":"2024-04-21T12:04:21.200405Z","iopub.status.idle":"2024-04-21T12:04:21.215624Z","shell.execute_reply.started":"2024-04-21T12:04:21.200371Z","shell.execute_reply":"2024-04-21T12:04:21.214789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/albeniz'  \nnotes = get_notes(midi_folder)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:04:21.216838Z","iopub.execute_input":"2024-04-21T12:04:21.217123Z","iopub.status.idle":"2024-04-21T12:04:29.367465Z","shell.execute_reply.started":"2024-04-21T12:04:21.217098Z","shell.execute_reply":"2024-04-21T12:04:29.366659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:04:29.36871Z","iopub.execute_input":"2024-04-21T12:04:29.369013Z","iopub.status.idle":"2024-04-21T12:04:29.563051Z","shell.execute_reply.started":"2024-04-21T12:04:29.368986Z","shell.execute_reply":"2024-04-21T12:04:29.562075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\"\"\"\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    \"\"\"Builds the VAE model and also returns the encoder.\"\"\"\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    # Instantiate the decoder model\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    # Loss function: binary crossentropy and KL divergence\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    # Return both the VAE and encoder models\n    return vae, encoder\n\nlatent_dim = 1000  # This should match the GAN's latent dimension\nsequence_length = 100  # This should match the length of your music sequences\n\n# Use the modified function to build the VAE and get the encoder\nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\n\n# Save the encoder model\nvae_encoder.save('albeniz_vae_encoder.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:04:29.564486Z","iopub.execute_input":"2024-04-21T12:04:29.56487Z","iopub.status.idle":"2024-04-21T12:04:32.621023Z","shell.execute_reply.started":"2024-04-21T12:04:29.564836Z","shell.execute_reply":"2024-04-21T12:04:32.620259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:04:32.622135Z","iopub.execute_input":"2024-04-21T12:04:32.622443Z","iopub.status.idle":"2024-04-21T12:09:20.785035Z","shell.execute_reply.started":"2024-04-21T12:04:32.622417Z","shell.execute_reply":"2024-04-21T12:09:20.784059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.save('/kaggle/working/albeniz_vae_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:09:20.788397Z","iopub.execute_input":"2024-04-21T12:09:20.788693Z","iopub.status.idle":"2024-04-21T12:09:20.966695Z","shell.execute_reply.started":"2024-04-21T12:09:20.788668Z","shell.execute_reply":"2024-04-21T12:09:20.965744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:09:20.967856Z","iopub.execute_input":"2024-04-21T12:09:20.968155Z","iopub.status.idle":"2024-04-21T12:09:44.097801Z","shell.execute_reply.started":"2024-04-21T12:09:20.968129Z","shell.execute_reply":"2024-04-21T12:09:44.096787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/albenizgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'albeniz_generated_music_{}'.format(i))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:09:44.099413Z","iopub.execute_input":"2024-04-21T12:09:44.099716Z","iopub.status.idle":"2024-04-21T12:09:44.109194Z","shell.execute_reply.started":"2024-04-21T12:09:44.09969Z","shell.execute_reply":"2024-04-21T12:09:44.10813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bach","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/bach\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_bach.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"bachgenerator_model.h5\")\n    gan.discriminator.save(\"bachdiscriminator_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:36:41.455434Z","iopub.execute_input":"2024-04-21T12:36:41.456135Z","iopub.status.idle":"2024-04-21T12:36:42.525241Z","shell.execute_reply.started":"2024-04-21T12:36:41.456101Z","shell.execute_reply":"2024-04-21T12:36:42.524372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/bach'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('bach_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/bach_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/bachgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'bach_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Balakir","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/balakir\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_balakir.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"balakirgenerator_model.h5\")\n    gan.discriminator.save(\"balakirdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/balakir'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('balakir_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/balakir_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/balakirgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'balakir_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Beeth","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/beeth\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_beeth.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"beethgenerator_model.h5\")\n    gan.discriminator.save(\"beethdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/beeth'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('beeth_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/beeth_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/beethgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'beeth_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Borodin","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/borodin\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_borodin.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"borodingenerator_model.h5\")\n    gan.discriminator.save(\"borodindiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/borodin'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('borodin_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/borodin_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/borodingenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'borodin_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Brahms","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/brahms\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_brahms.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"brahmsgenerator_model.h5\")\n    gan.discriminator.save(\"brahmsdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/brahms'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('brahms_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/brahms_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/brahmsgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'brahms_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Burgm","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/burgm\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_burgm.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"burgmgenerator_model.h5\")\n    gan.discriminator.save(\"burgmdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/burgm'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('burgm_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/burgm_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/burgmgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'burgm_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chopin","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/chopin\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_chopin.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"chopingenerator_model.h5\")\n    gan.discriminator.save(\"chopindiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/chopin'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('chopin_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/chopin_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/chopingenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'burgm_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/chopingenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'chopin_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Debussy","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/debussy\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_debussy.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"debussygenerator_model.h5\")\n    gan.discriminator.save(\"debussydiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/debussy'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('debussy_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/debussy_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/debussygenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'debussy_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Granados","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/granados\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_granados.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"Granadosgenerator_model.h5\")\n    gan.discriminator.save(\"Granadosdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/granados'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('Granados_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/Granados_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/Granadosgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'Granados_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grieg","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/grieg\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_grieg.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"grieggenerator_model.h5\")\n    gan.discriminator.save(\"griegdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/grieg'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('grieg_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/grieg_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/grieggenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'grieg_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Haydn","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/haydn\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_haydn.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"haydngenerator_model.h5\")\n    gan.discriminator.save(\"haydndiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/haydn'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('haydn_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/haydn_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/haydngenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'haydn_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# liszt","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/liszt\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_liszt.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"lisztgenerator_model.h5\")\n    gan.discriminator.save(\"lisztdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/liszt'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('liszt_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/liszt_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/lisztgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'liszt_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mendelssohn","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/mendelssohn\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_mendelssohn.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"mendelssohngenerator_model.h5\")\n    gan.discriminator.save(\"mendelssohndiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/mendelssohn'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('mendelssohn_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/mendelssohn_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/mendelssohngenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'mendelssohn_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mozart","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/mozart\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_mozart.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"mozartgenerator_model.h5\")\n    gan.discriminator.save(\"mozartdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/mozart'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('mozart_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/mozart_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/mozartgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'mozart_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# muss","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/muss\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_muss.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"mussgenerator_model.h5\")\n    gan.discriminator.save(\"mussdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/muss'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('muss_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/muss_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/mussgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'muss_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# schubert","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/schubert\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_schubert.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"schubertgenerator_model.h5\")\n    gan.discriminator.save(\"schubertdiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/schubert'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('schubert_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/schubert_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/schubertgenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'schubert_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# schumann","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/schumann\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_schumann.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"schumanngenerator_model.h5\")\n    gan.discriminator.save(\"schumanndiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/schumann'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('schumann_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/schumann_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/schumanngenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'schumann_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tschai","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nSEQUENCE_LENGTH = 100\nLATENT_DIMENSION = 1000\nBATCH_SIZE = 16\nEPOCHS = 10\nSAMPLE_INTERVAL = 1\n\n# Define the legacy optimizer\nlegacy_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n\ndef get_notes():\n    \"\"\" Get all the notes and chords from the midi files \"\"\"\n    notes = []\n\n    for file in Path(\"/kaggle/input/7th-april-generated-dataset/augmented_dataset/tschai\").glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n\ndef prepare_sequences(notes, n_vocab):\n    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n    sequence_length = 100\n\n    # Get all pitch names\n    pitchnames = sorted(set(item for item in notes))\n\n    # Create a dictionary to map pitches to integers\n    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n\n    network_input = []\n    network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(0, len(notes) - sequence_length, 1):\n        sequence_in = notes[i:i + sequence_length]\n        sequence_out = notes[i + sequence_length]\n        network_input.append([note_to_int[char] for char in sequence_in])\n        network_output.append(note_to_int[sequence_out])\n\n    n_patterns = len(network_input)\n\n    # Reshape the input into a format compatible with LSTM layers\n    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n    \n    # Normalize input between -1 and 1\n    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras\n\n    return network_input, network_output  # Add this return statement\n\n  \ndef create_midi(prediction_output, filename):\n    \"\"\" convert the output from the prediction to notes and create a midi file\n        from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # create note and chord objects based on the values generated by the model\n    for item in prediction_output:\n        pattern = item[0]\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\nclass GAN():\n    def __init__(self, rows):\n        self.seq_length = rows\n        self.seq_shape = (self.seq_length, 1)\n        self.latent_dim = 1000\n        self.disc_loss = []\n        self.gen_loss =[]\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', optimizer=legacy_optimizer, metrics=['accuracy'])\n        self.generator = self.build_generator()\n        z = Input(shape=(self.latent_dim,))\n        generated_seq = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(generated_seq)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss='binary_crossentropy', optimizer=legacy_optimizer)\n    def build_discriminator(self):\n        model = Sequential()\n        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n        model.add(Bidirectional(LSTM(512)))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(100))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        seq = Input(shape=self.seq_shape)\n        validity = model(seq)\n        return Model(seq, validity)\n    def build_generator(self):\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n        model.add(Reshape(self.seq_shape))\n        model.summary()\n        noise = Input(shape=(self.latent_dim,))\n        seq = model(noise)\n        return Model(noise, seq)\n    def train(self, epochs, batch_size=128, sample_interval=50):\n        notes = get_notes()\n        n_vocab = len(set(notes))\n        X_train, y_train = prepare_sequences(notes, n_vocab)\n        real = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_seqs = X_train[idx]\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_seqs = self.generator.predict(noise)\n            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, real)\n            if epoch % sample_interval == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n                self.disc_loss.append(d_loss[0])\n                self.gen_loss.append(g_loss)\n        self.generate(notes)\n        self.plot_loss()\n        \n    def generate(self, input_notes):\n        notes = input_notes\n        pitchnames = sorted(set(item for item in notes))\n        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n        noise = np.random.normal(0, 1, (1, self.latent_dim))\n        predictions = self.generator.predict(noise)\n        pred_notes = [x*242+242 for x in predictions[0]]\n        pred_notes_mapped = []\n        for x in pred_notes:\n            index = int(x)\n            if index in int_to_note:\n                pred_notes_mapped.append(int_to_note[index])\n            else:\n                pred_notes_mapped.append('C5')         \n        create_midi(pred_notes_mapped, 'gan_final')\n\n        \n    def plot_loss(self):\n        plt.plot(self.disc_loss, c='red')\n        plt.plot(self.gen_loss, c='blue')\n        plt.title(\"GAN Loss per Epoch\")\n        plt.legend(['Discriminator', 'Generator'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.savefig('GAN_Loss_per_Epoch_tschai.png', transparent=True)\n        plt.close()\n\nif __name__ == '__main__':\n    gan = GAN(rows=SEQUENCE_LENGTH)\n    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n\n    # Save the generator and discriminator models\n    gan.generator.save(\"tschaigenerator_model.h5\")\n    gan.discriminator.save(\"tschaidiscriminator_model.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom music21 import converter, instrument, note, chord, stream\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\ndef get_notes(midi_folder):\n    \"\"\" Get all the notes and chords from the midi files in the specified folder \"\"\"\n    notes = []\n\n    # Replace '/path/to/midi/folder' with the path to your folder containing MIDI files\n    for file in Path(midi_folder).glob(\"*.mid\"):\n        midi = converter.parse(file)\n\n        print(\"Parsing %s\" % file)\n\n        notes_to_parse = None\n        try:  # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[0].recurse()\n        except:  # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n\n    return notes\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi_folder = '/kaggle/input/7th-april-generated-dataset/augmented_dataset/tschai'  \nnotes = get_notes(midi_folder)\nn_vocab = len(set(notes))\nnetwork_input, network_output = prepare_sequences(notes, n_vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\ndef build_vae(latent_dim, sequence_length, n_vocab):\n    # Encoder model\n    inputs = Input(shape=(sequence_length, 1), name='encoder_input')\n    x = LSTM(512, return_sequences=True)(inputs)\n    x = LSTM(256)(x)\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n    # Sampling layer\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # Instantiate the encoder model\n    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # Decoder model\n    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n    x = RepeatVector(sequence_length)(latent_inputs)\n    x = LSTM(256, return_sequences=True)(x)\n    x = LSTM(512, return_sequences=True)(x)\n    outputs = Dense(1, activation='tanh')(x)\n\n    decoder = Model(latent_inputs, outputs, name='decoder')\n\n    # VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = Model(inputs, outputs, name='vae_mlp')\n\n    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n    reconstruction_loss *= sequence_length\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n\n    return vae, encoder\nlatent_dim = 1000  \nsequence_length = 100  \nvae, vae_encoder = build_vae(latent_dim, sequence_length, n_vocab)\nvae_encoder.save('tschai_vae_encoder.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE)\nvae.save('/kaggle/working/tschai_vae_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\n\n# Load the trained VAE encoder model\nvae_encoder = vae_encoder\n\ndef train_gan_with_vae(gan, vae_encoder, epochs, batch_size=128, sample_interval=50):\n    # Load and convert the data\n    notes = get_notes(midi_folder)  # This function needs to be defined to load your MIDI data\n    n_vocab = len(set(notes))\n    X_train, _ = prepare_sequences(notes, n_vocab)  # This function needs to be defined to preprocess your MIDI data\n\n    # Adversarial ground truths\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        # Select a random batch of note sequences\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_seqs = X_train[idx]\n\n        # Generate a batch of latent vectors using the VAE's encoder\n        # Only use the third output (z)\n        latent_vectors = vae_encoder.predict(real_seqs)[2]\n\n        # Generate a batch of new note sequences using the GAN's generator\n        gen_seqs = gan.generator.predict(latent_vectors)\n\n        # Train the discriminator\n        d_loss_real = gan.discriminator.train_on_batch(real_seqs, real)\n        d_loss_fake = gan.discriminator.train_on_batch(gen_seqs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        g_loss = gan.combined.train_on_batch(latent_vectors, real)\n\n        # Print the progress and save generated samples at specified intervals\n        if epoch % sample_interval == 0:\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            # Optionally, save the model and generated samples here\n\n\n# Create an instance of the GAN class\ngan_instance = GAN(rows=sequence_length)\n\n# Train the GAN using the VAE's encoder\ntrain_gan_with_vae(gan_instance, vae_encoder, epochs=10, batch_size=32, sample_interval=10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom music21 import instrument, note, chord, stream\n\n# Load the trained models\ngenerator = load_model('/kaggle/working/tschaigenerator_model.h5')\nencoder = vae_encoder\n\ndef generate_latent_vectors(encoder, num_samples, sequence_length):\n    # Generate random sequences as input for the encoder\n    random_sequences = np.random.normal(0, 1, (num_samples, sequence_length, 1))\n    # Predict the latent vectors\n    latent_vectors = encoder.predict(random_sequences)[2]\n    return latent_vectors\n\ndef generate_music(generator, latent_vectors, int_to_note, num_notes=100):\n    # Generate new sequences\n    generated_sequences = generator.predict(latent_vectors)\n    \n    # Convert sequences to notes\n    generated_notes = []\n    for seq in generated_sequences:\n        seq_notes = []\n        for note_value in seq:\n            rounded_note = int(np.round(note_value))\n            # Ensure the note is within the valid range of notes\n            if rounded_note in int_to_note:\n                seq_notes.append(int_to_note[rounded_note])\n            else:\n                # Handle out-of-range notes, e.g., by using a default note or ignoring them\n                seq_notes.append('C5')  # Example: default to 'C5'\n        generated_notes.append(seq_notes)\n    return generated_notes\n\n\ndef create_midi(prediction_output, filename):\n    \"\"\" Convert the output from the prediction to notes and create a midi file from the notes \"\"\"\n    offset = 0\n    output_notes = []\n\n    # Create note and chord objects based on the values generated by the model\n    for pattern in prediction_output:\n        # Pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # Pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # Increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n    midi_stream.write('midi', fp='{}.mid'.format(filename))\n\n# Define your int_to_note mapping here\nint_to_note = {number: note for number, note in enumerate(sorted(set(item for item in notes)))}\n\n# Generate latent vectors\nlatent_vectors = generate_latent_vectors(encoder, num_samples=10, sequence_length=100)\n\n# Generate music sequences\ngenerated_notes = generate_music(generator, latent_vectors, int_to_note)\n\n# Create MIDI files from the generated sequences\nfor i, notes in enumerate(generated_notes):\n    create_midi(notes, 'tschai_generated_music_{}'.format(i))\n","metadata":{},"execution_count":null,"outputs":[]}]}